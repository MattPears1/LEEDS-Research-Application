---
#####################
##  output format  ##
#####################
# The lines below makes the 'knit' button build the entire thesis 
# The format options are: 'pdf', 'bs4', 'gitbook', 'word'
# E.g. you can build both pdf and html with 'thesis_formats <- c("pdf", "bs4")'
knit: (function(input, ...) {
    thesis_formats <- c("bs4", "pdf", "word");
    
    source("scripts_and_filters/knit-functions.R");
    knit_thesis(input, thesis_formats, ...)
  })

#####################
##  metadata ##
#####################
title: |
  `Feasibility and Acceptance of Chatbots Embedded in Healthcare Curricula`: 
    
author: Matthew Pears
college: Research Fellow application
university:  ------------
university-logo: templates/w.png
university-logo-width: 5cm
submitted-text:
degree:  January
degreedate: 2023
dedication: Made in RMarkdown
acknowledgements: |
  `r paste(readLines("front-and-back-matter/_acknowledgements.Rmd"), collapse = '\n  ')`
show-acknowledgements-in-toc: false
abstract-heading: "Abstract"
abstract: |
  `r paste(readLines("front-and-back-matter/_abstract.Rmd"), collapse = "\n  ")`
show-abstract-in-toc: false
# add a second abstract with abstract-second-heading: "Abstract two", and abstract-second: "My abstract"
abbreviations: |
  `r paste(readLines("front-and-back-matter/_abbreviations.Rmd"), collapse = '\n  ')`

#######################
## bibliography path ##
#######################
bibliography: [bibliography/references.bib, bibliography/additional-references.bib]

########################
## PDF layout options ###
#########################
### submitting a master's thesis ###
# set masters-submission: true for an alternative, anonymous title page with 
# candidate number and word count
masters-submission: false
candidate-number: 1
word-count: "0"


### abbreviations ###
abbreviations-width: 3.2cm
abbreviations-heading: List of Abbreviations


### citation and bibliography style ###
# the title for the references section is created by front-and-back-matter/99-references_heading.Rmd, 
# but you can set it from here for convenience
params:
  referenceHeading: "References"

# citation and reference options (pandoc) #
csl: bibliography/apa.csl # try csl: bibliography/transactions-on-computer-human-interaction.csl for numerical citations
refs-line-spacing: 6mm
refs-space-between-entries: 1mm

## biblatex options ##
# use-biblatex: true # set to true, and set "output:bookdown::pdf_book: citation_package: biblatex"
# bib-latex-options: "style=authoryear, sorting=nyt, backend=biber, maxcitenames=2, useprefix, doi=true, isbn=false, uniquename=false" #for science, you might want style=numeric-comp, sorting=none for numerical in-text citation with references in order of appearance

## natbib options ##
# use-natbib: true # set to true, and set "output:bookdown::pdf_book: citation_package: natbib"
# natbib-citation-style: authoryear #for science, you might want numbers,square
# natbib-bibliography-style: templates/ACM-Reference-Format.bst #e.g. "plainnat", unsrtnat, or path to a .bst file


### correction highlighting ###
corrections: true

### link highlighting ###
border-around-links: false # false = links have colored text; true = links have a border around them

# Set the link text/border coloring here, in RGB. 
# If printing a physical version of your thesis, you'll want to comment out all of these.
urlcolor-rgb: "0,0,139"     # web addresses
citecolor-rgb: "0,33,71"    # citations
linkcolor-rgb: "0,0,139"    # links to sections in your thesis

# Make page number, not text, be link in TOC, LOF, and LOT. Otherwise, section link
# highlighting may look a bit excessive.
toc-link-page-numbers: true

### binding / margins ###
page-layout: nobind #'nobind' for equal margins (PDF output), 'twoside' for two-sided binding (mirror margins and blank pages), leave blank for one-sided binding (left margin > right margin)

### position of page numbers ###
#C = center, R = right, L = left. If page layout is 'twoside', O = odd pages and E = even pages. E.g. RO,LE puts the page number to the right on odd pages and left on even pages
ordinary-page-number-foot-or-head: foot #'foot' puts page number in footer, 'head' in header
ordinary-page-number-position: C
chapter-page-number-foot-or-head: foot #you may want it to be different on the chapter pages
chapter-page-number-position: C

### position of running header ###
#C = center, R = right, L = left. If page layout is 'twoside', O = odd pages and E = even pages. E.g. RO,LE puts the page number to the right on odd pages and left on even pages
running-header: true #indicate current chapter/section in header?
running-header-foot-or-head: head
running-header-position-leftmark: LO #marks the chapter. If layout is 'nobind', only this is used.
running-header-position-rightmark: RE  #marks the section.


### draft mark ###
draft-mark: false # add a DRAFT mark?
draft-mark-foot-or-head: foot ##'foot' = in footer, 'head' = in header
draft-mark-position: C
draft-mark-text: DRAFT on \today

### section numbering ###
section-numbering-depth: 2 # to which depth should headings be numbered?

### tables of content ###
table-of-contents: true # should there be one?
toc-depth: 1 # to which depth should headings be included in table of contents?
lof: true # include list of figures in front matter?
lot: true # include list of tables in front matter?
remove-mini-toc: false  # no mini-table of contents at start of each chapter? (for them to show up, this must be calse, and you must also add \minitoc after the chapter titles)
add-mini-lot: false  # include mini-list of tables by start of each chapter?
add-mini-lof: false  # include mini-list of figures by start of each chapter?

### code block spacing ###
space-before-code-block: 10pt
space-after-code-block: 8pt

### linespacing ###
linespacing: 22pt plus2pt # 22pt is official for submission & library copies
frontmatter-linespacing: 17pt plus1pt minus1pt #spacing in roman-numbered pages (acknowledgments, table of contents, etc.)

### title page
title-page: true
title-size: 22pt
title-size-linespacing: 28pt
gap-before-crest: 25mm
gap-after-crest: 25mm

### other stuff ###
abstractseparate: false  # include front page w/ abstract for examination schools?
includeline-num: false # show line numbering in PDF?
no-line-wrapping-in-code: false # by default, we prevent lines in code blocks from going off into the margins -- set 'true' to override this


#####################
## output details  ##
#####################
output:
  bookdown::pdf_book:
    template: templates/template.tex
    latex_engine: xelatex
    keep_tex: true
    pandoc_args: "--lua-filter=scripts_and_filters/colour_and_highlight.lua"
  bookdown::bs4_book: 
    css: 
      - templates/bs4_style.css
      - templates/corrections.css # remove to stop highlighting corrections
    theme:
      primary: "#6D1919"
    repo: E:\oxforddown-master
    pandoc_args: "--lua-filter=scripts_and_filters/colour_and_highlight.lua"
  bookdown::gitbook:
    css: templates/style.css
    config:
      sharing:
        facebook: false
        twitter: yes
        all: false
  bookdown::word_document2:
    toc: true   
link-citations: true
documentclass: book
always_allow_html: true #this allows html stuff in word (.docx) output
---

```{r install_packages, include=FALSE}
source('scripts_and_filters/install_packages_if_missing.R')
```

```{r create_chunk_options, include=FALSE, eval=knitr::is_latex_output()}
source('scripts_and_filters/create_chunk_options.R')
```

```{=html}
<!--
Include the create_chunk_options chunk above at the top of your index.Rmd file
This will include code to create additional chunk options (e.g. for adding author references to savequotes)
If you need to create your own additional chunk options, edit the file scripts/create_chunk_options.R
-->
```
<!-- This chunk includes the front page content in HTML output -->

```{r ebook-welcome, child = 'front-and-back-matter/_welcome-ebook.Rmd', eval=knitr::is_html_output()}
```

<!--chapter:end:index.Rmd-->

---
output:
  bookdown::html_document2: default
  bookdown::pdf_document2:
    template: templates/template.tex
  bookdown::word_document2: default
documentclass: book


#bibliography: [bibliography/references.bib, bibliography/additional-references.bib]
---

<style>
body {
text-align: justify}
</style>

# Introduction {.unnumbered}

```{=tex}
\adjustmtc
\markboth{Introduction}{}
```
<!-- For PDF output, include these two LaTeX commands after unnumbered chapter headings, otherwise the mini table of contents and the running header will show the previous chapter -->



## Background {#sec-background .unnumbered}

i wont repeat the application but i am sure you get the drift of the rmarkdown, to allow easier report writing for our work:

eg

1. (Essential to the job)
Demonstrable experience of working with extended reality technologies; *  

1.	For Leeds Hospitals Charity (LHC) I created a VR and PC Non-technical skills medical training application. See attached document “NTS Summary Report” for more details- which was the report submitted to the LHC December 2022. 

This resource is called an immersive Reusable Learning Object (iRLO). The design and development process followed a well- evidenced and effective framework called the ASPIRE framework. I performed everything from recordings and assets to the unity app, experimental setup and analysis, reporting/publishing etc. 
<br>

2.	I spent 5 years on my PhD which involved how surgical trainees deploy their attention in virtual environments to cues relating to patient safety. I made a VR app in Unreal, and combined expert insight to assess and train undergraduate ODP trainees.

<br>

3.	I worked on the CoViRR (Co-Creation of Virtual Reality Reusable E-Resources for European Healthcare Education) ERASMUS+ strategic partnership which co-created virtual reality reusable e-resources promoting innovative practices in digital era, by supporting current curricula and fostering open education. CoViRR intervened where current digital health training still lacks in solving many issue.

----i can add graphs and do statistical analyses etc. take a look at https://cepeh-analytics.netlify.app/ that i am just creating, and https://helm-analytics.netlify.app/ which i am analysing 100,000+ data




<!--chapter:end:00-introduction.Rmd-->

---
output:
  bookdown::html_document2: default
  bookdown::pdf_document2:
    template: templates/template.tex
  bookdown::word_document2: default
documentclass: book
bibliography: [bibliography/references.bib, bibliography/additional-references.bib]
---

<style>
  body {text-align: justify}
 body {background-color: lavenderblush;}
    pre, pre:not([class]) { background-color: lavenderblush;}
</style>


```{r Importing Data, message=FALSE, warning=FALSE, include=FALSE, paged.print=TRUE}
knitr::opts_chunk$set(
	echo = FALSE,
	message = FALSE,
	warning = FALSE
)

required_packages <- c("rmarkdown", "bookdown", "knitr", "kableExtra", "tidyverse", "here", "readxl", "ggplot2", "lubridate", "plotly", "dplyr", "wesanderson", "viridis","leaflet")

for (package in required_packages) {
  print(paste0("checking for install of ", package))
  if (!requireNamespace(package)) install.packages(package, repos = "http://cran.rstudio.com")
}

library(readxl)
library(ggplot2)
library(lubridate)
library(plotly)
library(dplyr)
#color pallets
library(wesanderson)
library(viridis)
library(knitr)
library(tidyverse)
library(tinytex)
library(leaflet)

try(data <- read_excel("C:/Users/MattP/Desktop/Full DATA CEPEH Moh.xlsx"))

sex <- data$sex
sex <- data.frame(sex)

disc <- subset(data, select = "Location")
disc <- na.omit(disc)
colnames(disc)[1] = "Location"

```

# this would be the methods section etc etc

\minitoc <!-- this will include a mini table of contents-->

## Procedure
## Participants etc etc





<!--chapter:end:01-Method.Rmd-->

---
output:
  bookdown::pdf_document2:
    template: templates/template.tex
  bookdown::html_document2: default
  bookdown::word_document2: default
documentclass: book
bibliography: [bibliography/references.bib, bibliography/additional-references.bib]

editor_options: 
  markdown: 
    wrap: 72
  chunk_output_type: console
---

<style>
body {text-align: justify}
 body {background-color: lavenderblush;}
    pre, pre:not([class]) { background-color: lavenderblush;}
  </style>


# Results {#rmd-basics}

\minitoc <!-- this will include a mini table of contents-->

<!-- LaTeX normally does not indent the first line after a heading - however, it does so after the mini table of contents. You can manually tell it not to with \noindent -->

\noindent

```{r Importing Data 2, message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}

knitr::opts_chunk$set(
	echo = FALSE,
	message = FALSE,
	warning = FALSE
)

required_packages <- c("rmarkdown", "bookdown", "knitr", "kableExtra", "tidyverse", "here", "readxl", "ggplot2", "lubridate", "plotly", "dplyr", "wesanderson", "viridis","leaflet")

for (package in required_packages) {
  if (!requireNamespace(package)) install.packages(package, repos = "http://cran.rstudio.com")
}

library(readxl)
library(ggplot2)
library(lubridate)
library(plotly)
library(dplyr)
#color pallets
library(wesanderson)
library(viridis)
library(knitr)
library(tidyverse)
library(tidytext)
library(leaflet)
library(excelR)


try(data <- read_excel("C:/Users/MattP/Desktop/Full DATA CEPEH Moh - Copy.xlsx"))

sex <- data$sex
sex <- data.frame(sex)

disc <- subset(data, select = "Location")
disc <- na.omit(disc)
colnames(disc)[1] = "Location"
```

## Introduction

Hi there, I thought to create this site using R, Github and a web builder just to show I have at least a basic understanding. 
```{r previous chatbot use, echo=FALSE, message=FALSE, warning=FALSE}

# table for previous chatbot usage
prevchat <- subset(data, select = c("Previous_Chatbot_Usage"))

knitr::kable(prevchat <-prevchat %>% filter(Previous_Chatbot_Usage %in% c("Never","1-4 hours","5-9 hours","20+ hours","10-19 hours",format('html'))) %>% group_by(Previous_Chatbot_Usage)%>% count(),"pipe",decreasing = TRUE)
```

Table: Previous Chatbot Usage of Participants  
<br><br>  

In short, approximately 50% had never used a chatbot, and 45% had used a
chatbot, at some period over the years, for a short period of time.  

Most learners use books or online books as resources. They may use multiple sources however they were asked to note the primary source. Only 6 stated their primary sources were *Online videos/interactive materials* which includes such tools as chatbots.  

```{r Boxplotsplits2, echo=FALSE, fig.cap="Chatbot Usage agreements- Pre", message=FALSE, warning=FALSE}

library(tidyverse)
library(readr)

Boxplotsplits2 <- read.csv("C:/Users/MattP/Desktop/Full DATA CEPEH Moh - Copy.csv", header=TRUE, na.strings=c("", "NA"))

#------make ggplot this way, instead of making a dataset, this is better as you can just filter and skips steps
Boxplotsplits2 %>%
  
  filter(Profession %in% c("Healthcare_Student","Doctor","Learning Technologist","College student","Postgraduate student", "Medical doctor","Lecturer","Mature Student")) %>%
           
           
#------then pipe into ggplot
  #the first aesthetic label is always the x axis
  ggplot(aes(Profession))+
  geom_bar(aes(fill = Profession), alpha = 0.8)+
  facet_wrap(~Use_chatbots_frequently_pre)+
  theme_get()+
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank())+
                 labs(title="I would like to use chatbots frequently- Pre",
                      x= "Like to use chatbots frequently (Pre)",
                      y= "Frequency")
```

The first boxplot (\@ref(fig:Boxplotsplits2)) shows the intention, or at least the learners interest in using chatbots by means of agreeing that they would like to use chatbots if they had the opportunity. 20 healthcare students agreed (16) and strongly agreed (4) which made about 50% of participants. 9 were neutral with 1 disagree.

```{r BoxplotUsefulPre, echo=FALSE, fig.cap="Chatbots are Useful Opinion- Pre", message=FALSE, warning=FALSE, paged.print=TRUE}

library(tidyverse)
library(readr)
library(ggcharts)

try(Boxplotsplits2 <- read_excel("C:/Users/MattP/Desktop/Full DATA CEPEH Moh.xlsx"))


#------make ggplot this way, instead of making a dataset, this is better as you can just filter and skips steps
Boxplotsplits2 %>%
  
  filter(UsefulPE1PRE %in% c("Strongly Agree","Agree","Neutral","Disagree","Strongly Disagree")) %>%
           
#------then pipe into ggplotggplot(aes(ChatbotUsed))+
  
  #the first aesthetic label is always the x axis
  ggplot(aes(UsefulPE1PRE))+
  geom_bar(aes(fill = Profession), alpha = 0.8)+
  scale_y_continuous(labels = function(x) paste0(x * 2.5, '%'))+
 facet_wrap(~UsefulPE1PRE)+
  theme_get()+ 
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        axis.title.x=element_blank(),
        axis.text.x=element_blank())+
                 labs(title="     I think healthcare chatbots are useful- Pre",
                      x= "Like to use chatbots frequently (Pre)",
                      y= "Frequency")

```

(\@ref(fig:BoxplotUsefulPre)) shows the opinions of all participants on the usefulness of chatbots. Many had not had experience with them yet had positive rating. 

This positive opinions of chatbots may be from colleagues, friends, media, tutors, or other social information of the benefits in healthcare education. Around 25% were neutral or disagreed that healthcare chatbots were useful.

<br><br>

***The participants then used the 4 chatbots and completed the post-usage survey after each chatbot. Results after use are as followed:***  

## Chatbot Usability Questionnaire (CUQ)

### CUQ Calculation tool

The CUQ was developed by researchers at Ulster University,
[Link](https://www.ulster.ac.uk/research/topic/computer-science/artificial-intelligence/projects/cuq)
and as the calculation can be complex, a dedicated calculation tool has been created.

Please download the CEPEH CUQ calculation tool which has all of the data entered, so you can see the CEPEH CUQ scoring

[Click here to download CUQ calc tool](CUQ-Calculation-Tool.xlsx)

[Click here to download CEPEH CUQ score result](cuq.png)
  
  
```{r cuqimage, echo=FALSE, fig.align='center', fig.cap="CUQ CEPEH Score", message=FALSE, warning=FALSE, out.width="75%", paged.print=TRUE}
knitr::include_graphics("cuq.png")
```

Although the design and development was similar, each chatbot CUQ score was calculated to understand how the topic content may affect usability:

The breakdown of the chatbots was:

-   Aristotle University of Thessaloniki CUQ score = 63/100
-   CYENS Centre of Excellence CUQ score = 67/100
-   Karolinska Institute CUQ score = 63/100
-   University of Nottingham CUQ score = 68/100  


The score for all 3 chatbots grouped was 65/100. See Discussion CUQ
section for interpretation

```{r CUQscatterplot, echo=FALSE, fig.align='center', fig.cap="CUQ Scatter Plot", message=FALSE, warning=FALSE, paged.print=FALSE}
library(readr)
mydata.needed <- read_csv("Full DATA CEPEH Moh - Copy.csv")

attach(mydata.needed)
plot(CUQ,
     main="Chatbot Usability Questionnaire Scoring",
     xlab="Participant responce data (4 chatbots)",
     ylab="Scoring out of 100",
     col=factor(ChatbotUsed))
      legend("bottomright",
       legend = levels(factor(ChatbotUsed)),
       pch = 19,
       col = factor(levels(factor(ChatbotUsed))))
```

Figure (\@ref(fig:CUQscatterplot)) shows the CUQ scores as a scatter plot to highlight how there was a moderate distribution of results.
Further exploration is required to understand which elements are causing this spread, and if it was due to problems within a small group of learners.
<br><br>


## System Usability Scale (SUS) Questions

*Note= The amount of 'agreement' is defined as the addition of 'Agree'
and 'Strongly agree' responses.*

The SUS score should consist of 10 items. However, some SUS questions were improved upon by 1 or more CUQ questions, specifically to this Chatbot study. The SUS results would be obscured by the CUQ scores, expect 2 that did not have cross-over. The two questions were:

-   I would like to use the CEPEH chatbot I tested, more frequently
    
-   I felt confident using the CEPEH chatbot 


This meant the score of the SUS was not created, however the CUQ score better represented the Learners' perceptions of the CEPEH chatbot in terms of feasibility of use and acceptability in healthcare curricula.

```{r SUSkeepusing,fig.cap="Keep using CEPEH chatbots", echo=FALSE,fig.align='center', message=FALSE, warning=FALSE, paged.print=FALSE}
# table for sus results
SUS2 <- subset(data, select = c("I would like to use the CEPEH chatbot i tested, more frequently (SUS1)(post)", "I felt confident using the CEPEH chatbot (SUS2)(post)"))
SUS2 <- na.omit(SUS2)
colnames(SUS2)[1] = "Keep Using CEPEH Chatbot"
colnames(SUS2)[2] = "Confident"

library(plyr)

counts <- ddply(SUS2, .(SUS2$"Keep Using CEPEH Chatbot"), nrow)
names(counts) <- c("Keep Using CEPEH Chatbot", "Responses")
knitr::kable((counts),"pipe")

```

The table \@ref(tab:SUSkeepusing) above shows the results for agreement participants may
continue to use the CEPEH chatbots: 89/126 (70%) agreed or strongly agreed. However, there were 23 records that learners were neutral or disagree they would continue use.

```{r confidence, echo=FALSE,fig.cap="Confidence in CEPEH chatbots", fig.align='center', message=FALSE, warning=FALSE, paged.print=FALSE, tab.cap="confidence"}

counts <- ddply(SUS2, .(SUS2$Confident), nrow)
names(counts) <- c("Keep Using", "V1")

colnames(counts)[1] = "Confidence using CEPEH Chatbot(s)"
colnames(counts)[2] = "Responses"

knitr::kable((counts),"pipe")

```

Confidence when using the chatbots is in table (\@ref(tab:confidence))- it shows the distribution of agreement for participants for all
4 chatbots. The table shows 90/126 records that participants feel they are confident in using the chatbots. However, 21/126 (16%) were neutral and 11/126 (8.5%) disagreed and this was explored in the qualitative analysis section.

## Technology Acceptance Model

The TAM questions were analysed according to their subsets. The subsets
were Perceived Usefulness (PU) and Perceived Ease of Use (PEU)

The questions were-

Perceived Usefulness (PU):
1.  Using CEPEH chatbots would enable me to accomplish tasks more
    quickly
2.  Using CEPEH chatbots would increase performance
3.  Using CEPEH chatbots would increase my productivity
4.  I would find CEPEH chatbots useful on my course

Perceived Ease of Use (PEU):
5.  Learning to use CEPEH chatbots would be easy to me
6.  It would be easy for me to be skilful at using CEPEH chatbots
7.  My interactions with CEPEH chatbots would be clear and understandable
8.  I would find CEPEH chatbots easy to use

The scores as a percentage of agreement, were calculated by averaging the subsets and interpreted as:

-   Before using the CEPEH chatbots, there was 66% (2.2/5) agreement for the Perceived Usefulness of chatbots in healthcare education, and after 48% (2.6/5) agreed.

-   Before using the CEPEH chatbots, there was 64% (2.3) agreement for Perceived Ease of Use of chatbots in healthcare education, and after 51% (2.56) agreed.

The justification for this may be due to being early versions of applications with limited functionality and functions which can be difficult for user to experience the intended further range of features and learning exercises.  
    
  

### Knowledge and Trust after Use


CYENS chatbot had around 10 more participants stating that they were neutral on gaining knowledge of the topic. The ﬁgure 2.6 shows the ratings by participants of the CEPEH Chatbots to provide them with the necessary course information.

```{r Boxplot knowledge, echo=FALSE, fig.cap="Improvements in Knowledge", fig.align='center',message=FALSE, warning=FALSE}

library(tidyverse)
library(readr)

Boxplotsplits5 <- read.csv("C:/Users/MattP/Desktop/Full DATA CEPEH Moh - Copy.csv", header=TRUE, na.strings=c("", "NA"))

#------make ggplot this way, instead of making a dataset, this is better as you can just filter and skips steps
Boxplotsplits5 %>%
  
  filter(knowledge_improved_after_use%in% c("Strongly Agree","Agree","Neutral","Disagree","Strongly Disagree")) %>%
           
           drop_na(knowledge_improved_after_use) %>%
  drop_na(knowledge_improved_after_use)%>%
#------then pipe into ggplot
  #the first aesthetic label is always the x axis
  ggplot(aes(ChatbotUsed))+
  geom_bar(aes(fill = ChatbotUsed), alpha = 0.8)+
  facet_wrap(~knowledge_improved_after_use)+
  theme_get()+
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank())+
                 labs(title="My Knowledge of the Topic Improved after Use",
                      x= "",
                      y= "Frequency")
```


The figure (\@ref(fig:Boxplot trust)) shows the ratings by participants of the CEPEH Chatbots to provide them with the necessary course information.

```{r Boxplot trust, echo=FALSE, fig.cap="Trust Chatbots POST use",fig.align='center', message=FALSE, warning=FALSE}

library(tidyverse)
library(readr)

Boxplotsplits6 <- read.csv("C:/Users/MattP/Desktop/Full DATA CEPEH Moh - Copy.csv", header=TRUE, na.strings=c("", "NA"))

#------make ggplot this way, instead of making a dataset, this is better as you can just filter and skips steps
Boxplotsplits6 %>%
  
  filter(POST_Trust%in% c("Strongly Agree","Agree","Neutral","Disagree","Strongly Disagree")) %>%
           
           drop_na(POST_Trust) %>%
  drop_na(POST_Trust)%>%
#------then pipe into ggplot
  #the first aesthetic label is always the x axis
  ggplot(aes(ChatbotUsed))+
  geom_bar(aes(fill = ChatbotUsed), alpha = 0.8)+
  facet_wrap(~POST_Trust)+
  theme_get()+
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank())+
                 labs(title="I Trust CEPEH Chatbots to Provide me with my Course Information",
                      x= "",
                      y= "Frequency")
```

This is a integral element in learners' motivational and educational choices to reuse the learning resources. As previously described, the trust of the information is also a factor in these responses.






## Personality and Interactions

```{r Boxplotsplits8, echo=FALSE, message=FALSE, fig.align='center', warning=FALSE}

library(tidyverse)
library(readr)

Boxplotsplits8 <- read.csv("C:/Users/MattP/Desktop/Full DATA CEPEH Moh - Copy.csv", header=TRUE, na.strings=c("", "NA"))

#------make ggplot this way, instead of making a dataset, this is better as you can just filter and skips steps
Boxplotsplits8 %>%
  
  filter(Robotic_CUQ2_post %in% c("Strongly Agree","Agree","Neutral","Disagree","Strongly Disagree")) %>%
           
           drop_na(Robotic_CUQ2_post) %>%
  drop_na(Robotic_CUQ2_post)%>%
#------then pipe into ggplot
  #the first aesthetic label is always the x axis
  ggplot(aes(ChatbotUsed))+
  geom_bar(aes(fill = ChatbotUsed), alpha = 0.8)+
  facet_wrap(~Robotic_CUQ2_post)+
  theme_get()+
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank())+
                 labs(title="The chatbot seemed too robotic (CUQ2)(post)",
                      x= "",
                      y= "Frequency")
```

*The chatbot seemed too robotic* results had the largest mix of
responses, and for all 4 chatbots evaluated. The University of
Nottingham Cybersecurity chatbot had more deterministic pathways with
exploitation of the NLP modelling to provide illusion of realism. This
may explain why there was less agreement. However, Neutrality and/or
agreement was not desired.


```{r Boxplot personality, echo=FALSE, fig.align='center', message=FALSE, warning=FALSE}

library(tidyverse)
library(readr)

Boxplotsplits7 <- read.csv("C:/Users/MattP/Desktop/Full DATA CEPEH Moh - Copy.csv", header=TRUE, na.strings=c("", "NA"))

#------make ggplot this way, instead of making a dataset, this is better as you can just filter and skips steps
Boxplotsplits7 %>%
  
  filter(Personailty_real_engage %in% c("Strongly Agree","Agree","Neutral","Disagree","Strongly Disagree")) %>%
           
           drop_na(Personailty_real_engage) %>%
  drop_na(Personailty_real_engage)%>%
#------then pipe into ggplot
  #the first aesthetic label is always the x axis
  ggplot(aes(ChatbotUsed))+
  geom_bar(aes(fill = ChatbotUsed), alpha = 0.8)+
  facet_wrap(~Personailty_real_engage)+
  theme_get()+
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank())+
                 labs(title="CEPEH Chatbot Personailty was Realistic and Engaging",
                      x= "",
                      y= "Frequency")
```

 
There were mixed results for the chatbot used being realistic and engaging. This question has two descriptive terms however based on the other results we understand that the chatbots’ NLP logic, or ability to respond required improvement to be more ‘smooth’ in  replying. The primary limitation was found in the ‘robotic’ interactions (See Figure x). This was
investigated further in the ‘Text Mining’ and ‘Sentiment Analysis’ sections.


## Ease of Use and Seeking Support

```{r Boxplotsplits9, echo=FALSE, fig.cap="Ease of Use Comparison", fig.align='center', message=FALSE, warning=FALSE}

library(tidyverse)
library(readr)

Boxplotsplits9 <- read.csv("C:/Users/MattP/Desktop/Full DATA CEPEH Moh - Copy.csv", header=TRUE, na.strings=c("", "NA"))

#------make ggplot this way, instead of making a dataset, this is better as you can just filter and skips steps
Boxplotsplits9 %>%
  
  filter(CEPEH_Post_Easy_To_Use %in% c("Strongly Agree","Agree","Neutral","Disagree","Strongly Disagree")) %>%
           
           drop_na(CEPEH_Post_Easy_To_Use) %>%
  drop_na(Easy_To_Use_Pre)%>%
#------then pipe into ggplot
  #the first aesthetic label is always the x axis
  ggplot(aes(Easy_To_Use_Pre))+
  geom_bar(aes(fill = Easy_To_Use_Pre), alpha = 0.8)+
  facet_wrap(~CEPEH_Post_Easy_To_Use)+
  theme_get()+
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        axis.title.x=element_blank(),
        axis.text.x=element_blank())+
                 labs(title="Change in Ease of Use Perception, after CEPEH Chatbot Usage",
                      x= "",
                      y= "Frequency")
```

After usage, there was only agreement in Ease of Use- as shown in
(\@ref(fig:Boxplotsplits9) as there are no 'Neutral' or disagree
columns. Any learners with disagreement before using the CEPEH chatbots,
after believed they were easy to use.

```{r Boxplotsplits10, echo=FALSE, fig.cap="Ease of Use Comparison", message=FALSE, warning=FALSE}

library(tidyverse)
library(readr)

Boxplotsplits10 <- read.csv("C:/Users/MattP/Desktop/Full DATA CEPEH Moh - Copy.csv", header=TRUE, na.strings=c("", "NA"))

#------make ggplot this way, instead of making a dataset, this is better as you can just filter and skips steps
Boxplotsplits10 %>%
  
  filter(get_help %in% c("Strongly Agree","Agree","Neutral","Disagree","Strongly Disagree")) %>%
           
           drop_na(get_help) %>%
  drop_na(get_help_pre)%>%
#------then pipe into ggplot
  #the first aesthetic label is always the x axis
  ggplot(aes(get_help_pre))+
  geom_bar(aes(fill =get_help_pre), alpha = 0.8)+
  facet_wrap(~get_help)+
  theme_get()+
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank())+
                 labs(title="I can get help when I have difficulties using CEPEH chatbots",
                      x= "",
                      y= "Frequency")
```

Those who disagreed or were neutral in the pre usage measure, improved
their understanding that help was available with the CEPEH chatbots.
After usage, 40 participants agreed they could get help if they had
difficulty using the resources.





```{r accomplishquickly, echo=FALSE, fig.align='center', fig.cap="Pre-post accomplish quickly", message=FALSE, warning=FALSE, paged.print=FALSE}
knitr::include_graphics("clear.png")
```


```{r ttest, echo=FALSE, fig.align='center', fig.cap="Table of T-test results", message=FALSE, warning=FALSE, paged.print=FALSE}
knitr::include_graphics("table ttest.png")
```


```{r clearpic, echo=FALSE, fig.align='center', fig.cap="pre-post clear", message=FALSE, warning=FALSE, paged.print=FALSE}
knitr::include_graphics("clear.png")
```


```{r bigtable, echo=FALSE, fig.align='center', fig.cap="Table of Results", fig.height=6, fig.width=5, message=FALSE, warning=FALSE, out.height="125%", paged.print=FALSE}
knitr::include_graphics("bigtable.png")
```
This rather large table presents all of the descriptive statistic measures AFTER chatbot usage. The Mode is important to show that the majority of participants stated their perceptions, experience, and acceptance increased. 


```{r second setup stuff 3, message=FALSE, warning=FALSE, comment=NA, include=FALSE, paged.print=FALSE}

library(readxl)
library(ggplot2)
library(lubridate)
library(plotly)
library(dplyr)
#color pallets
library(wesanderson)
library(viridis)
library(knitr)
library(tidyverse)


```



## Inferential Statistics


Paired t-test involves matching the same participants on a variable before intervention with, ideally, the equal measure of variable after intervention. For this study, we used several metrics from the various questionnaires to facilitate pre-post comparisons. The CUQ was only asked after chatbot usage, however most other questions were able to have a pre-post comparison.

Importantly, there is value in the modes which indicate majority consensus, rather than mean driven t-tests. Being an initial single session with technical orientation for Users as well as practical usage, there is high change that a minority will experience technical or functional problems. Although the majority may benefit, the measures from this minority can significantly impact the measures. For example, if a sample of 42 have the results 5/5 (27), 4/5 (11), and 1/5 (5), the mode and median are 5 however the mean is 4.38. This can affect parametric or non-parametric results that infer equal experience from participant. We are not overlooking participants but factoring in their experiences as a minority and assessing their results in other ways- i.e., the focus group discussions. This better reflects each participants experience and the accuracy of efficacy of the chatbots. 
Because of the experimental set-up and the 4 different chatbots, the meaning of the t-tests has small power and effect size. We intended for all metrics to improve, but to have significant findings with the setup does not provide much more information in addition to increased means/scores. We performed paired t-tests, however many paired samples had significant Shapiro-Wilk results indicating the normality assumption were violated. The test is robust to violations, and the same participants are being tested over Time, rather than different participants. This also indicated that the meaning of the Wilcoxon has limited strength. 




### Paired sample t-test and Wilcoxon signed rank test


Two-tailed paired t-tests were conducted to examine whether the mean difference is of the following groups were significantly different:

•	Confidence
•	 Daily usefulness
•	Increasing achievements
•	Accomplishing things quickly
•	Increased productivity
•	Ease of use
•	Clear and understandable interactions
•	Use chatbots more frequently

The results showed there were no significant differences in these comparisons. For each comparison the results were:

•	Confidence- t(24) = -0.35, **p = .731** (prem=1.96, postm=2.04)

•	Daily usefulness- V = 48.50, z = -0.27, **p = .790** (prem=2.04, postm=2.12)

•	Increasing achievements- t(24) = -0.18, **p = .857** (prem=2.28, postm=2.32)

•	Accomplish tasks quickly- V = 36.00, z = -0.25, **p = .805** (prem=2.12, postm=2.16)

•	Increased productivity- V = 96.00, z = -1.51, **p = .131** (prem=2.6, postm=2.24)

•	Ease of use- V = 72.00, z = -1.32, **p = .186** (prem=2.36, postm=2.12)

•	Clear and understandable interactions- V = 101.50, z = -1.82, **p = .068** (prem=2.2, postm=2.61)

•	Use chatbots more frequently- t(24) = 0.45, **p = .657** (prem=2.2, postm=2.08)


As predicted, the sensitivity of the t-test meant Wilcoxon test was more appropriate for some measures. The results show minor increases in means for most, but minor decreases in others. These results have high standard deviations which are from a minority of participants scoring low, and explored in the focus group discussions.



```{r Boxplotsplits11, echo=FALSE, fig.cap="Intend to Reuse by Profession-Post", message=FALSE, warning=FALSE}

library(tidyverse)
library(readr)

Boxplotsplits40 <- read.csv("C:/Users/MattP/Desktop/Full DATA CEPEH Moh - Copy.csv", header=TRUE, na.strings=c("", "NA"))
      

#------make ggplot this way, instead of making a dataset, this is better as you can just filter and skips steps
Boxplotsplits40 %>%
  
filter(Intend %in% c("Agree","Neutral","Disagree","Strongly Agree","Strongly Disagree")) %>%
 drop_na(Profession) %>%
#------then pipe into ggplot
  #the first aesthetic label is always the x axis
  ggplot(aes(Profession))+
  geom_bar(aes(fill = Profession), alpha = 0.8)+
  facet_wrap(~Intend)+
  theme_get()+
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        
        axis.ticks.x=element_blank())+
                 labs(title="I Intend to again use the CEPEH Chatbots",
                      x= "Like to use chatbots frequently (Pre)",
                      y= "Frequency")
```
<br>
After using the CEPEH chatbots, majority of participants stated they
would reuse the chatbots. However, there was 6 counts of *disagree* or
*strongly disagree* for all 4 chatbots. Further, there were 17 counts of
neutral in reuse, which was approximately 4 participants per chatbot
(see (\@ref(fig:Boxplotsplits4)).

```{r Boxplotsplits4, fig.cap="Intend to Reuse-Post", echo=FALSE, message=FALSE, warning=FALSE}

library(tidyverse)
library(readr)

Boxplotsplits4 <- read.csv("C:/Users/MattP/Desktop/Full DATA CEPEH Moh - Copy.csv", header=TRUE, na.strings=c("", "NA"))

#------make ggplot this way, instead of making a dataset, this is better as you can just filter and skips steps
Boxplotsplits4 %>%
  
  filter(Intend %in% c("Strongly Agree","Agree","Neutral","Disagree","Strongly Disagree")) %>%
           
           drop_na(Intend) %>%
  
#------then pipe into ggplot
  #the first aesthetic label is always the x axis
  ggplot(aes(ChatbotUsed))+
  geom_bar(aes(fill = ChatbotUsed), alpha = 0.8)+
  facet_wrap(~Intend)+
  theme_get()+
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank())+
                 labs(title="I Intend to again use the CEPEH Chatbots",
                      x= "",
                      y= "Frequency")
```

For CYENS, even though the knowledge of the topic was not perceived to
improve by some participants, this box plot shows how 34/42 stated they
would reuse the chatbot developed by CYENS.
<br><br>

```{r Boxplotsplits3, fig.cap="Easy to Use- Post", echo=FALSE, message=FALSE, warning=FALSE}

library(tidyverse)
library(readr)

Boxplotsplits3 <- read.csv("C:/Users/MattP/Desktop/Full DATA CEPEH Moh - Copy.csv", header=TRUE, na.strings=c("", "NA"))

#------make ggplot this way, instead of making a dataset, this is better as you can just filter and skips steps
Boxplotsplits3 %>%
  
  filter(Information_with_minimal_command_post %in% c("Strongly Agree","Agree","Neutral","Disagree","Strongly Disagree")) %>%
           
           drop_na(Information_with_minimal_command_post) %>%
  drop_na(Information_with_minimal_command_post)%>%
#------then pipe into ggplot
  #the first aesthetic label is always the x axis
  ggplot(aes(ChatbotUsed))+
  geom_bar(aes(fill = ChatbotUsed), alpha = 0.8)+
  facet_wrap(~Information_with_minimal_command_post)+
  theme_get()+
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank())+
                 labs(title="CEPEH Chatbots are Easy to Use",
                      x= "Like to use chatbots frequently (Pre)",
                      y= "Frequency")
```

There was only 1 'Strongly Disagree' response. The agreement options
counted for the majority of the data.
Repeated Measures t-test, aka paired t-test (before and after
measurements)
<br>
```{r T-test, message=FALSE, warning=FALSE, include=FALSE}

library(readr)

mydata.needed <- read_csv("Full DATA CEPEH Moh - Copy.csv")

attach(mydata.needed)
names(mydata.needed)
mydata.needed[1:3]

plot(CUQ)

t.test(CUQ, after, mu=0, alt="two.sided", paired=T, conf.level=0.095)

```

This t-test compares confident using mobile chatbots before and after
CEPEH chatbot usage.


<!--chapter:end:02-Results.Rmd-->

---


output:
  bookdown::pdf_document2:
    template: templates/template.tex
  bookdown::html_document2: default
  bookdown::word_document2: default
documentclass: book
#bibliography: [bibliography/references.bib, bibliography/additional-references.bib]
editor_options: 
  markdown: 
    wrap: 72
# Text Mining, Natural Language Processing, and Sentiment Analysis
---

<style>
  body {text-align: justify}
 body {background-color: azure;}
    pre, pre:not([class]) { background-color: azure;}
</style>


```{=html}
<style>
body {
text-align: justify}
</style>
```
# and here is a bit of text mining i am doing, basic frequency 

```{r setup packages, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(
	echo = FALSE,
	message = TRUE,
	warning = TRUE
)
library(tidyverse) #for various data manipulation tasks
library(tidytext) #for text mining specifically, main package in book
library(stringr) #for various text operations
library(gutenbergr) #to access full-text books that are in the public domain
library(scales) # for visualising percentages
library(readtext) # for reading in txt files
library(wordcloud) # for creating wordclouds

library(syuzhet)
```

The focus group discussions provided a lot of feedback for how the
participants experienced their interactions with the chatbots, and how
the CEPEH team can improve them, improve the design and development
processes, and improve uptake and sharing.

One method of analysing this data is with use of text mining and data
manipulation, creating word clouds, sentiment analysis, and using a
model which can distinguish the unique themes in text, and highlights
for us what text is used to create these themes.

Therefore, we have created a model to allow efficient and intelligent
analysis of this open/free focus group data.

```{r read text and name it, echo=FALSE, message=FALSE, warning=FALSE}
CEPEHQ_raw <- readtext("P1.txt")

CEPEHQ_raw $doc_id <- sub(".txt", "", CEPEHQ_raw $doc_id) # this gets rid of .txt in the play titles
```

```{r batch read files, eval=FALSE, include=FALSE}
CEPEHQ_raw <- readtext("Shakespeare txts/*")

```

```{r rename file, message=FALSE, warning=FALSE, include=FALSE}
CEPEHQ_raw$doc_id <- as.factor(CEPEHQ_raw$doc_id)
CEPEHQ_raw$doc_id <- plyr::revalue(CEPEHQ_raw$doc_id,
                                              c("P1" = "1"))
```

## Tokenising

Firstly, we tokenised the words from the FGDs. A Token is "a meaningful
unit of text, most often a word, that we are interested in using for
further analysis". For each word we give it a property that we can call
upon later.

The data manipulation for this included removing punctuation, converting
to lower-case, and setting word type to word (and not such types as
"characters", "ngrams", "sentences", "lines" etc)

```{r give text tokens, message=FALSE, warning=FALSE, include=FALSE}
CEPEHQ_tidy <- CEPEHQ_raw%>% 
  unnest_tokens(word, text)

head(CEPEHQ_tidy)

```

```{r echo=FALSE, message=FALSE, warning=FALSE}
CEPEHQ_tidy <- CEPEHQ_tidy %>% 
  mutate(word = str_extract(word, "[a-z']+"))
```

### Stop words

The model then removed words with meaningless function. These are called
stop words. Words like "the", "of" and "to" are the most frequent words
found, technically, but are of little interest to us.

```{r message=FALSE, warning=FALSE, include=FALSE}
stop_words
```

```{r message=FALSE, warning=FALSE, include=FALSE}
CEPEHQ_tidy <- CEPEHQ_tidy %>% 
  anti_join(stop_words)

CEPEHQ_tidy
```

We also created a custom list of stop words for CEPEH. We know
participants may mention other objects, and the list was as followed:
found; chatbot; chatbots; presentation.

```{r include=FALSE}
meaningless_words <- tibble(word = c("found", "chatbot", "cybersecurity","chatbots", "presentations", "presentation", "NA", "video", "videos", "didnt", "didn't", "na", "nil", ""))

CEPEHQ_tidy <- CEPEHQ_tidy %>% 
  anti_join(meaningless_words)
```

The data was ready for analysis by the model. We ordered it to find the
most frequent words. Below is a table with the 6 frequently occurring
words, showing how Stop words have now been filtered.

```{r this half worked but keep, echo=FALSE, message=FALSE, warning=FALSE}

library(tidytext)
library(tidyverse)
library(tm)
library(tigerstats)
library(tidyselect)
library(dplyr)


CEPEHQ_freq <- CEPEHQ_tidy %>% count(word,sort=TRUE)
knitr::kable(head(CEPEHQ_freq),"pipe")

CEPEHQ_freq <- CEPEHQ_tidy %>% group_by(doc_id) %>% count(word,sort=TRUE)

CEPEHQ_freq <- CEPEHQ_freq[-6,]


```

This word list can then be used for sentiment analysis, (see *Sentiment
Analysis* section), in addition to frequency of words.

```{r group by word and filter for true, eval=FALSE, message=FALSE, warning=FALSE, include=FALSE, paged.print=TRUE}
CEPEHQ_tidy %>% 
  group_by(doc_id) %>% 
  count(word, sort=TRUE)
sample(CEPEHQ_tidy)
```

## Plotting word frequencies - bar graphs

```{r FREQ words bar graph, fig.align='center', fig.cap="CEPEH", message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
CEPEHQ_freq %>% 
  filter(n>7& doc_id == "1") %>% 
  ggplot(aes(x=word, y=n)) +
  geom_col()
```

```{r Readable labels, message=FALSE, warning=FALSE, include=FALSE}
CEPEHQ_freq %>% 
  filter(n>7 & doc_id == "1") %>% 
  ggplot(aes(x=word, y=n)) +
  geom_col() +
  theme(axis.text.x = element_text(angle = 45))
```

```{r Descending order, message=FALSE, warning=FALSE, include=FALSE}
CEPEHQ_freq %>% 
  filter(n>7 & doc_id == "1") %>% 
  ggplot(aes(x=reorder(word, -n), y=n)) +
  geom_col() +
  theme(axis.text.x = element_text(angle = 45))
```

```{r message=FALSE, warning=FALSE, include=FALSE}
CEPEHQ_freq <- na.omit(CEPEHQ_freq)

CEPEHQ_freq %>%
  filter(n>4 & doc_id == "1") %>% 
  ggplot(aes(x=reorder(word, -n), y=n, fill=n)) +
  geom_col(show.legend=FALSE) +
  theme(axis.text.x = element_text(angle = 45)) +
  xlab("Word") +
  ylab("Frequency") +
  ggtitle("Most frequent words in CEPEH Chatbot Focus Group Discussions")
```

```{r fig.align='center', message=FALSE, warning=FALSE, include=FALSE}
CEPEHQ_freq %>% 
  filter(n>3,doc_id == "1") %>% na.omit(CEPEHQ_freq)%>%
  ggplot(aes(x=reorder(word, n), y=n, fill=n)) +
  geom_col(show.legend=FALSE) +
  xlab("Word") +
  ylab("Frequency") +
  ggtitle("Most frequent words in all CEPEH qualatative data") 
```

### Normalised frequency

With this information a list of top words from the participants in the
FGD can be rendered and after some modifications, a graph of the top 20
words is produced, with better aesthetics. This is a better way to
understand this data, and the axis can be normalised for the frequency
of occurrences in accordance with the source text. The raw text had 2827
words in total. Therefore we can mutate the ratios to reflect this.

```{r message=FALSE, warning=FALSE, include=FALSE}
# see the total number of words per play (doc_id)
CEPEHQ_freq %>% 
  group_by(doc_id) %>% 
  mutate(sum(n)) %>% 
  distinct(doc_id, sum(n))

CEPEHQ_freq <- CEPEHQ_freq %>% 
  na.omit() %>% 
  group_by(doc_id) %>% 
  mutate(WordsPerSource = n*2827/sum(n)) %>% # creates a new column called pmw
  ungroup() %>% 
  anti_join(stop_words) # removing stopwords afterwards

CEPEHQ_freq %>% select(word, WordsPerSource)
```

### Plotting normalised frequency

Now we can plot, for example, the 20 most frequent words when normalised
by the source text.

```{r CEPEH MOST FREQ, echo=FALSE,fig.align='center', message=FALSE, warning=FALSE}
CEPEHQ_freq %>% 
  filter(doc_id == "1") %>% 
  top_n(20, WordsPerSource) %>% 
  ggplot(aes(x=reorder(word, +WordsPerSource), y=WordsPerSource, fill=WordsPerSource)) +
  geom_col(show.legend=TRUE) +
  theme(axis.text.x = element_text(angle = 45)) +
  xlab("Word") +
  ylab("Frequency within 2827 words FGD Source") +
  ggtitle("Most frequent words in CEPEH focus group data") +
  coord_flip()


```

In summary, this understanding of frequent words can help to understand
common concurrences and extrapolate to a larger audience. If scope and
impact of CEPEH chatbots increased we can understand the type of themes
and trends may occur, based on such FGD analysis.

## Word clouds

To visualise the most frequent words in another format, below is a word
cloud which presents the word size to indicate the frequency- words that
occur more often being displayed in a larger font size. This has a
normalised data frequency in accordance to the FGD source document
analysed.

```{r echo=FALSE, message=FALSE, fig.align='center', warning=FALSE}
wordcloud(words = CEPEHQ_freq$word, freq = CEPEHQ_freq$n, 
          min.freq = 4, max.words=2000, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))
```

We understand the context has been reduced for each word. However, in
general there can be categorised positive/negative words from the word
cloud: Positive words are- benefit, practical, nice, helpful, learn,
ideas, and enjoyed Negative words are- difficult, test (who likes a
test?), don't, and 'lot' may be negative if there is a 'lot' of
information.

### The vocabulary of Texts

```{r include=FALSE}
comp_2 <- CEPEHQ_freq %>% 
  filter(doc_id == "1"|doc_id == "1") %>% 
  group_by(doc_id) %>% 
  mutate(proportion = n / sum(n)) %>% #creates proportion column (word frequency divided by overall frequency per author)
  select(-n) %>%
  spread(doc_id, proportion)

summary(comp_2)
```

Here is a graph that has plotted the words in places depending on the
word frequencies. Additionally, colour hotspots shows how different the
frequencies are - darker items are more similar in terms of their
frequencies, lighter-coloured ones more frequent in one text compared to
the other.

```{r echo=FALSE, message=FALSE,fig.align='left', warning=FALSE}
ggplot(comp_2, 
       aes(x = `1`, y = `1`, 
           color = abs(`1` - `1`))) +
  geom_abline(color = "Black", lty = 2) +
  geom_jitter(alpha = 0.3, size = 3.5, width = 0.4, height = 0.8,color = "Purple") +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.9,color = "Black") +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format())+
  theme_light() +
  theme(legend.position="none") +
  labs(y = "Word Frequency as %", x = " ")

```

## Sentiment analysis

What is the sentiment of all participants? What is types of emotional
words are being used? The preparation of these words has some use in
understanding the frequencies, but their emotional valence are not
compared. The table above has the word *'helpful'* which has a positive
connotation, however there are 386 words, with many having several
occurrences.

```{r sentiment, message=FALSE, warning=FALSE, include=FALSE}
head(get_sentiments("bing"))

CEPEHQ_tidy %>% 
  inner_join(get_sentiments("bing")) %>% 
  head()

CEPEHQ_tidy %>% 
  inner_join(get_sentiments("bing"))%>% 
  count(sentiment)

CEPEHQ_tidy %>% 
  inner_join(get_sentiments("bing")) %>% 
  count(word, sentiment, sort = TRUE) %>% 
  head()

CEPEHQ_tidy %>% 
  inner_join(get_sentiments("bing")) %>% 
  count(doc_id,sentiment) %>% 
  head()


CEPEHQ_tidy  %>% 
  inner_join(get_sentiments("bing"))%>% 
  count(sentiment)

CEPEHQ_tidy  %>% 
  inner_join(get_sentiments("bing")) %>% 
  count(word, sentiment, sort = TRUE) %>% 
  head()

```

```{r message=FALSE, warning=FALSE, include=FALSE, paged.print=TRUE}

CEPEHQ_tidy_bing <-  CEPEHQ_tidy%>% 
  inner_join(get_sentiments("bing")) %>% 
  count(sentiment) %>% 
  spread(sentiment, n, fill=0) %>% 
  mutate(total_score = positive - negative)
```

```{r sentiment2, message=FALSE, warning=FALSE, include=FALSE, paged.print=TRUE}
CEPEHQ_tidy_bing %>% 
  summarize(max(total_score), min(total_score)) 



```


As the table below shows. the FGD data has been analysed for sentiment
of each word, and has been calculated to have 62 positive emotional
valence of words, with 24 negative valence of words. These are from a
**Bing sentiment lexicon** which is the most popular English language
dictionary.


```{r Sentiment score, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(CEPEHQ_tidy_bing) 
```


Unfortunately, there is little research using sentiment analysis for
chatbot related focus group results that can help to understand the
scoring found. However, on a basic interpretation the higher the score
the better the chatbots were discussed in the FGD's. A score of 72%
(62/(24+62))) would be in 3/4th quartile in distribution of sentiment
distribution. Alternatively, 62/24 = 2.58 would state the ratio that for
every 1 negative word recorded, there were 2.58 positive words recorded.


<!--chapter:end:03-TM.Rmd-->

---
output:
  #bookdown::html_document2: default
  #bookdown::word_document2: default
  bookdown::pdf_document2:
    template: templates/template.tex
    keep_tex: true
  bookdown::html_document2: default
  bookdown::word_document2: default
documentclass: book
---

# Discussion {#Discussion}

\minitoc <!-- this will include a mini table of contents-->

##Summary 

A quick setup using r git and site pushing just to show basisc skill. thanks

<!--chapter:end:04-Discussion.Rmd-->

output:
  #bookdown::html_document2: default
  #bookdown::word_document2: default
  bookdown::pdf_document2: 
    template: templates/template.tex
documentclass: book

# (Additional Analyses) Training Events {#cites-and-refs}

\chaptermark{Citations and cross-refs}
\minitoc <!-- this will include a mini table of contents-->


## CEPEH Training Event C1


The CEPEH training event C1 held at the premises of University of Nottingham aiming to prepare participants for the practical elements of co-creation and implementation of chatbots as an educational resource. It combined both theoretical and hands-on training.
15 participants were from RISE, AUTH, UoN.

Project managers of partners signposted the person involved, and relevant announcements were made though social media channels to the wider public. External to the project speakers were from University of Leeds, and Computer Science Department of University of Nottingham. It included academics, medical doctors, and researchers with focus both on clinical research and digital innovations in healthcare education and IT specialist/learning technologists 11.18 years of experiences (SD=7.2). A balance between male and female participants achieved.



Participants were asked to highlight what they liked for each day and how each day can be improved. Findings are described below per day of the training event


Day 1  
The participants comment that they liked the design method for educational resources presented using a co-creation approach, they liked the interactions with other groups, and they liked the overview of existing chatbot resources of the partners. On the areas that can be improved, more media material were requested.


Day 2
Participants enjoyed the presentation from the invited speaker from another faculty of the University of Nottingham, the CEPEH recources presented and the storyboarding process. Participants highlighted that the participation of more clinicians in the event would be an added value in regards with the storyboarding process.


Day3
Participants liked the hands-on activities of the day also enjoyed the creativity of the groups on the online chatbot development tool. As an area of improvement, participants wanted more time on hands on sections.



```{r map Importing Data, eval=FALSE, message=FALSE, warning=FALSE, include=FALSE, paged.print=TRUE}
try(museum <- read_excel("C:/Users/MattP/Desktop/Full DATA CEPEH Moh.xlsx"))
museum %>%
leaflet() %>%
addProviderTiles(providers$Esri.WorldImagery, group = "World Imagery") %>%
addProviderTiles(providers$Stamen.TonerLite, group = "Toner Lite") %>%
addLayersControl(baseGroups = c("Toner Lite", "World Imagery")) %>%
addMarkers(label = museum$museum,
           clusterOptions = markerClusterOptions(),
           popup = ifelse(museum$`Presidential Library`=="Yes",
                          "1 person around this area",
                          "1 person around this area")) %>% setView(lat = 53.921236496, lng = -1.1392187894608434, zoom = 09)
```


```{r MOHAMMED please make FREQ bar chart of top 15 Cities- sorted, eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}

city <- subset(data, select = "City")
city <- na.omit(city)

#table filter- to remove text from a coolumn:- !grep and then the cell text 

city <- subset(city, select = "City") %>%filter(!grepl('INVALID IPV4 ADDRESS', City))
city1 <- subset(city, select = "City") %>%filter(!grepl(' ', City))
view(city1)
colnames(city1)[1] = "City"

####insert table for frequency top 15 here

```



## CEPEH Training Event 2

**Pre-Training Event survey May 9th-13th 2022 Thessaloniki, Greece**

Twenty-six participants attended the Training Event, along with approximately 10 staff members. There were 21 undergraduate students and 5 postgraduate students, who completed the survey for a total of 26 responses. There were 86% of participants who stated they had not been to a similar event like the training event CEPEH facilitated. There were 90% of students who found the event schedule very organised, and 70% agreed most of the planned sessions were relevant to that interest with the remaining 30% not having enough experience to understand the context to determine if they are interested in the training event. There were 95% of students agreeing or strongly agreeing the training event location is great, the remaining person did not leave additional comments. 


Table 1 suggested attendees had minimal intention to share their own ideas due to lack of previous experience of attending such events, or due to lack of knowledge on the area. However, most were interested in listening to other groups and hearing contextual cases in healthcare.



There were 77% of participants stated they were novices in experience with chatbots in healthcare and were attending to learn more.  The remaining 23% (7 students) stated they were competent and had limited experience with chatbots in healthcare.


One day had several events regarding cybersecurity in healthcare. When asked before these events, 83% stated they were neutral or disagreed that they felt confident about their cybersecurity knowledge in healthcare. In addition, 80% stated they when neutral or disagreed that they felt they had strong cybersecurity safety in healthcare.  Table 2 shows the main pre and post results suggesting a positive experience for more than 75% of attendees on all measures. 



There were 90% (23) of students who heard about the event through a lecturer or a professor, the CEPEH newsletter (2), and 1 person was informed through the anatomy tutoring system at Karolinska Institute. Additionally, 60% suggested the training event to somebody else before the course started.


There were six individuals who stated neutral or disagree when asked if having issues on registration or finding the information for the event. This may have been due to being dependent on emails to receive the information, instead of a dedicated website where the information is available anytime. 


As this was face-to-face, participants were asked about sufficient Covid-19 precautions in place at the facility, 94% agreed with sufficient precautions, two individuals stated no but did not give further information in the additional input box provided.
In summary, most participants were undergraduate students with novice experience, happy with the training event location, felt the sessions were relevant to them, and most shared the event with their colleagues. The values of co-creation, chatbots in healthcare, and taking patient history were bestowed to students in an engaging and well-received manner. Notably, the highest ratings were for staff friendliness which is key to engagement and consistent interaction throughout the intense and long 5-day duration. The sessions were recorded there for the online recordings may be viewed with higher numbers over the subsequent weeks. 






# bibliography: [bibliography/references.bib, bibliography/additional-references.bib]
---

```{block type='savequote', include=knitr::is_latex_output(), quote_author='(ref:darwin-quote)'}
There is grandeur in this view of life, with its several powers, having been originally breathed into a few forms or into one; and that, whilst this planet has gone cycling on according to the fixed law of gravity, from so simple a beginning endless forms most beautiful and most wonderful have been, and are being, evolved.
```
(ref:darwin-quote) --- Charles Darwin [@Darwin1859]
<!-- note that the quote author won't show up when you knit just a single chapter -->
  

<!--chapter:end:05-extensions.Rmd-->

---
#########################################
# options for knitting a single chapter #
#########################################
output:
  #bookdown::html_document2: default
  #bookdown::word_document2: default
  bookdown::pdf_document2:
    template: templates/template.tex
documentclass: book
#bibliography: [bibliography/references.bib, bibliography/additional-references.bib]
---


```{block type='savequote', include=knitr::opts_knit$get('rmarkdown.pandoc.to') == 'latex', quote_author='(ref:goethe-quote)'}
Alles Gescheite ist schon gedacht worden.\
Man muss nur versuchen, es noch einmal zu denken.

All intelligent thoughts have already been thought;\
what is necessary is only to try to think them again.

<!-- ending a line with a lonely backslash inserts a linebreak -->
```
(ref:goethe-quote) --- Johann Wolfgang von Goethe [@von_goethe_wilhelm_1829]



# Appendix {-}

<!--chapter:end:06-conclusion.Rmd-->

`r if(knitr:::is_latex_output()) '\\startappendices'`

`r if(!knitr:::is_latex_output()) '# (APPENDIX) Appendix {-}'` 

<!-- If you feel it necessary to include an appendix, it goes here. The first appendix should include the commands above. -->


# The First Appendix

This first appendix includes an R chunk that was hidden in the document (using `echo = FALSE`) to help with readability:

**In 02-rmd-basics-code.Rmd**

```{r ref.label='chunk-parts', eval=FALSE, echo = TRUE}
```

**And here's another one from the same chapter, i.e. Chapter \@ref(code):**

```{r ref.label='oxford-logo-rotated', eval=FALSE, echo = TRUE}
```



<!--chapter:end:front-and-back-matter/98-appendices.Rmd-->

<!-- This .Rmd file serves only to add the References heading, irrespective of output, and irrespective of whether you are using biblatex, natbib, or pandoc for references -->

# `r params$referenceHeading` {-}

<!-- the below latex command is needed after unnumbered chapter headings -- without it, the running header will show the title of the previous chapter -->
```{=tex}
\markboth{`r params$referenceHeading`}{}
```

::: {#refs}
:::


<!--chapter:end:front-and-back-matter/99-references_heading.Rmd-->

