---


output:
  bookdown::pdf_document2:
    template: templates/template.tex
  bookdown::html_document2: default
  bookdown::word_document2: default
documentclass: book
#bibliography: [bibliography/references.bib, bibliography/additional-references.bib]
editor_options: 
  markdown: 
    wrap: 72
# Text Mining, Natural Language Processing, and Sentiment Analysis
---

<style>
  body {text-align: justify}
 body {background-color: azure;}
    pre, pre:not([class]) { background-color: azure;}
</style>


```{=html}
<style>
body {
text-align: justify}
</style>
```
# and here is a bit of text mining i am doing, basic frequency 

```{r setup packages, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(
	echo = FALSE,
	message = TRUE,
	warning = TRUE
)
library(tidyverse) #for various data manipulation tasks
library(tidytext) #for text mining specifically, main package in book
library(stringr) #for various text operations
library(gutenbergr) #to access full-text books that are in the public domain
library(scales) # for visualising percentages
library(readtext) # for reading in txt files
library(wordcloud) # for creating wordclouds

library(syuzhet)
```

The focus group discussions provided a lot of feedback for how the
participants experienced their interactions with the chatbots, and how
the CEPEH team can improve them, improve the design and development
processes, and improve uptake and sharing.

One method of analysing this data is with use of text mining and data
manipulation, creating word clouds, sentiment analysis, and using a
model which can distinguish the unique themes in text, and highlights
for us what text is used to create these themes.

Therefore, we have created a model to allow efficient and intelligent
analysis of this open/free focus group data.

```{r read text and name it, echo=FALSE, message=FALSE, warning=FALSE}
CEPEHQ_raw <- readtext("P1.txt")

CEPEHQ_raw $doc_id <- sub(".txt", "", CEPEHQ_raw $doc_id) # this gets rid of .txt in the play titles
```

```{r batch read files, eval=FALSE, include=FALSE}
CEPEHQ_raw <- readtext("Shakespeare txts/*")

```

```{r rename file, message=FALSE, warning=FALSE, include=FALSE}
CEPEHQ_raw$doc_id <- as.factor(CEPEHQ_raw$doc_id)
CEPEHQ_raw$doc_id <- plyr::revalue(CEPEHQ_raw$doc_id,
                                              c("P1" = "1"))
```

## Tokenising

Firstly, we tokenised the words from the FGDs. A Token is "a meaningful
unit of text, most often a word, that we are interested in using for
further analysis". For each word we give it a property that we can call
upon later.

The data manipulation for this included removing punctuation, converting
to lower-case, and setting word type to word (and not such types as
"characters", "ngrams", "sentences", "lines" etc)

```{r give text tokens, message=FALSE, warning=FALSE, include=FALSE}
CEPEHQ_tidy <- CEPEHQ_raw%>% 
  unnest_tokens(word, text)

head(CEPEHQ_tidy)

```

```{r echo=FALSE, message=FALSE, warning=FALSE}
CEPEHQ_tidy <- CEPEHQ_tidy %>% 
  mutate(word = str_extract(word, "[a-z']+"))
```

### Stop words

The model then removed words with meaningless function. These are called
stop words. Words like "the", "of" and "to" are the most frequent words
found, technically, but are of little interest to us.

```{r message=FALSE, warning=FALSE, include=FALSE}
stop_words
```

```{r message=FALSE, warning=FALSE, include=FALSE}
CEPEHQ_tidy <- CEPEHQ_tidy %>% 
  anti_join(stop_words)

CEPEHQ_tidy
```

We also created a custom list of stop words for CEPEH. We know
participants may mention other objects, and the list was as followed:
found; chatbot; chatbots; presentation.

```{r include=FALSE}
meaningless_words <- tibble(word = c("found", "chatbot", "cybersecurity","chatbots", "presentations", "presentation", "NA", "video", "videos", "didnt", "didn't", "na", "nil", ""))

CEPEHQ_tidy <- CEPEHQ_tidy %>% 
  anti_join(meaningless_words)
```

The data was ready for analysis by the model. We ordered it to find the
most frequent words. Below is a table with the 6 frequently occurring
words, showing how Stop words have now been filtered.

```{r this half worked but keep, echo=FALSE, message=FALSE, warning=FALSE}

library(tidytext)
library(tidyverse)
library(tm)
library(tigerstats)
library(tidyselect)
library(dplyr)


CEPEHQ_freq <- CEPEHQ_tidy %>% count(word,sort=TRUE)
knitr::kable(head(CEPEHQ_freq),"pipe")

CEPEHQ_freq <- CEPEHQ_tidy %>% group_by(doc_id) %>% count(word,sort=TRUE)

CEPEHQ_freq <- CEPEHQ_freq[-6,]


```

This word list can then be used for sentiment analysis, (see *Sentiment
Analysis* section), in addition to frequency of words.

```{r group by word and filter for true, eval=FALSE, message=FALSE, warning=FALSE, include=FALSE, paged.print=TRUE}
CEPEHQ_tidy %>% 
  group_by(doc_id) %>% 
  count(word, sort=TRUE)
sample(CEPEHQ_tidy)
```

## Plotting word frequencies - bar graphs

```{r FREQ words bar graph, fig.align='center', fig.cap="CEPEH", message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
CEPEHQ_freq %>% 
  filter(n>7& doc_id == "1") %>% 
  ggplot(aes(x=word, y=n)) +
  geom_col()
```

```{r Readable labels, message=FALSE, warning=FALSE, include=FALSE}
CEPEHQ_freq %>% 
  filter(n>7 & doc_id == "1") %>% 
  ggplot(aes(x=word, y=n)) +
  geom_col() +
  theme(axis.text.x = element_text(angle = 45))
```

```{r Descending order, message=FALSE, warning=FALSE, include=FALSE}
CEPEHQ_freq %>% 
  filter(n>7 & doc_id == "1") %>% 
  ggplot(aes(x=reorder(word, -n), y=n)) +
  geom_col() +
  theme(axis.text.x = element_text(angle = 45))
```

```{r message=FALSE, warning=FALSE, include=FALSE}
CEPEHQ_freq <- na.omit(CEPEHQ_freq)

CEPEHQ_freq %>%
  filter(n>4 & doc_id == "1") %>% 
  ggplot(aes(x=reorder(word, -n), y=n, fill=n)) +
  geom_col(show.legend=FALSE) +
  theme(axis.text.x = element_text(angle = 45)) +
  xlab("Word") +
  ylab("Frequency") +
  ggtitle("Most frequent words in CEPEH Chatbot Focus Group Discussions")
```

```{r fig.align='center', message=FALSE, warning=FALSE, include=FALSE}
CEPEHQ_freq %>% 
  filter(n>3,doc_id == "1") %>% na.omit(CEPEHQ_freq)%>%
  ggplot(aes(x=reorder(word, n), y=n, fill=n)) +
  geom_col(show.legend=FALSE) +
  xlab("Word") +
  ylab("Frequency") +
  ggtitle("Most frequent words in all CEPEH qualatative data") 
```

### Normalised frequency

With this information a list of top words from the participants in the
FGD can be rendered and after some modifications, a graph of the top 20
words is produced, with better aesthetics. This is a better way to
understand this data, and the axis can be normalised for the frequency
of occurrences in accordance with the source text. The raw text had 2827
words in total. Therefore we can mutate the ratios to reflect this.

```{r message=FALSE, warning=FALSE, include=FALSE}
# see the total number of words per play (doc_id)
CEPEHQ_freq %>% 
  group_by(doc_id) %>% 
  mutate(sum(n)) %>% 
  distinct(doc_id, sum(n))

CEPEHQ_freq <- CEPEHQ_freq %>% 
  na.omit() %>% 
  group_by(doc_id) %>% 
  mutate(WordsPerSource = n*2827/sum(n)) %>% # creates a new column called pmw
  ungroup() %>% 
  anti_join(stop_words) # removing stopwords afterwards

CEPEHQ_freq %>% select(word, WordsPerSource)
```

### Plotting normalised frequency

Now we can plot, for example, the 20 most frequent words when normalised
by the source text.

```{r CEPEH MOST FREQ, echo=FALSE,fig.align='center', message=FALSE, warning=FALSE}
CEPEHQ_freq %>% 
  filter(doc_id == "1") %>% 
  top_n(20, WordsPerSource) %>% 
  ggplot(aes(x=reorder(word, +WordsPerSource), y=WordsPerSource, fill=WordsPerSource)) +
  geom_col(show.legend=TRUE) +
  theme(axis.text.x = element_text(angle = 45)) +
  xlab("Word") +
  ylab("Frequency within 2827 words FGD Source") +
  ggtitle("Most frequent words in CEPEH focus group data") +
  coord_flip()


```

In summary, this understanding of frequent words can help to understand
common concurrences and extrapolate to a larger audience. If scope and
impact of CEPEH chatbots increased we can understand the type of themes
and trends may occur, based on such FGD analysis.

## Word clouds

To visualise the most frequent words in another format, below is a word
cloud which presents the word size to indicate the frequency- words that
occur more often being displayed in a larger font size. This has a
normalised data frequency in accordance to the FGD source document
analysed.

```{r echo=FALSE, message=FALSE, fig.align='center', warning=FALSE}
wordcloud(words = CEPEHQ_freq$word, freq = CEPEHQ_freq$n, 
          min.freq = 4, max.words=2000, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))
```

We understand the context has been reduced for each word. However, in
general there can be categorised positive/negative words from the word
cloud: Positive words are- benefit, practical, nice, helpful, learn,
ideas, and enjoyed Negative words are- difficult, test (who likes a
test?), don't, and 'lot' may be negative if there is a 'lot' of
information.

### The vocabulary of Texts

```{r include=FALSE}
comp_2 <- CEPEHQ_freq %>% 
  filter(doc_id == "1"|doc_id == "1") %>% 
  group_by(doc_id) %>% 
  mutate(proportion = n / sum(n)) %>% #creates proportion column (word frequency divided by overall frequency per author)
  select(-n) %>%
  spread(doc_id, proportion)

summary(comp_2)
```

Here is a graph that has plotted the words in places depending on the
word frequencies. Additionally, colour hotspots shows how different the
frequencies are - darker items are more similar in terms of their
frequencies, lighter-coloured ones more frequent in one text compared to
the other.

```{r echo=FALSE, message=FALSE,fig.align='left', warning=FALSE}
ggplot(comp_2, 
       aes(x = `1`, y = `1`, 
           color = abs(`1` - `1`))) +
  geom_abline(color = "Black", lty = 2) +
  geom_jitter(alpha = 0.3, size = 3.5, width = 0.4, height = 0.8,color = "Purple") +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.9,color = "Black") +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format())+
  theme_light() +
  theme(legend.position="none") +
  labs(y = "Word Frequency as %", x = " ")

```

## Sentiment analysis

What is the sentiment of all participants? What is types of emotional
words are being used? The preparation of these words has some use in
understanding the frequencies, but their emotional valence are not
compared. The table above has the word *'helpful'* which has a positive
connotation, however there are 386 words, with many having several
occurrences.

```{r sentiment, message=FALSE, warning=FALSE, include=FALSE}
head(get_sentiments("bing"))

CEPEHQ_tidy %>% 
  inner_join(get_sentiments("bing")) %>% 
  head()

CEPEHQ_tidy %>% 
  inner_join(get_sentiments("bing"))%>% 
  count(sentiment)

CEPEHQ_tidy %>% 
  inner_join(get_sentiments("bing")) %>% 
  count(word, sentiment, sort = TRUE) %>% 
  head()

CEPEHQ_tidy %>% 
  inner_join(get_sentiments("bing")) %>% 
  count(doc_id,sentiment) %>% 
  head()


CEPEHQ_tidy  %>% 
  inner_join(get_sentiments("bing"))%>% 
  count(sentiment)

CEPEHQ_tidy  %>% 
  inner_join(get_sentiments("bing")) %>% 
  count(word, sentiment, sort = TRUE) %>% 
  head()

```

```{r message=FALSE, warning=FALSE, include=FALSE, paged.print=TRUE}

CEPEHQ_tidy_bing <-  CEPEHQ_tidy%>% 
  inner_join(get_sentiments("bing")) %>% 
  count(sentiment) %>% 
  spread(sentiment, n, fill=0) %>% 
  mutate(total_score = positive - negative)
```

```{r sentiment2, message=FALSE, warning=FALSE, include=FALSE, paged.print=TRUE}
CEPEHQ_tidy_bing %>% 
  summarize(max(total_score), min(total_score)) 



```


As the table below shows. the FGD data has been analysed for sentiment
of each word, and has been calculated to have 62 positive emotional
valence of words, with 24 negative valence of words. These are from a
**Bing sentiment lexicon** which is the most popular English language
dictionary.


```{r Sentiment score, echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(CEPEHQ_tidy_bing) 
```


Unfortunately, there is little research using sentiment analysis for
chatbot related focus group results that can help to understand the
scoring found. However, on a basic interpretation the higher the score
the better the chatbots were discussed in the FGD's. A score of 72%
(62/(24+62))) would be in 3/4th quartile in distribution of sentiment
distribution. Alternatively, 62/24 = 2.58 would state the ratio that for
every 1 negative word recorded, there were 2.58 positive words recorded.

