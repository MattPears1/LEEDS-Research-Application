---
output: 
  html_document: 
    fig_caption: yes
---

output:
  #bookdown::html_document2: default
  #bookdown::word_document2: default
  bookdown::pdf_document2:
    template: templates/template.tex
documentclass: book

#bibliography: [bibliography/references.bib, bibliography/additional-references.bib]

# Text Mining, Natural Language Processing, and Sentiment Analysis

```{r setup packages, message=FALSE, warning=FALSE}
library(tidyverse) #for various data manipulation tasks
library(tidytext) #for text mining specifically, main package in book
library(stringr) #for various text operations
library(gutenbergr) #to access full-text books that are in the public domain
library(scales) # for visualising percentages
library(readtext) # for reading in txt files
library(wordcloud) # for creating wordclouds

library(syuzhet)
```


## CEPEH Qualatative Feedback

The focus group discussions provided a lot of feedback for how the participants experienced their interactions with the chatbots, and how the CEPEH team can improve them, improve the design and development processes, and improve uptake and sharing. 

One method of analysing this data is with use of text mining and data manipulation, creating word clouds, sentiment analysis, and using a model which can distinguish the unique themes in text, and highlights for us what text is used to create these themes.

Therefore, we have created such tools to allow efficient and intelligent analysis of this open/free focus group data.

```{r read text and name it}
CEPEHQ_raw <- readtext("P1.txt")

CEPEHQ_raw $doc_id <- sub(".txt", "", CEPEHQ_raw $doc_id) # this gets rid of .txt in the play titles
```

```{r batch read files, eval=FALSE, include=FALSE}
CEPEHQ_raw <- readtext("Shakespeare txts/*")

```

```{r rename file, echo=FALSE, message=FALSE, warning=FALSE}
CEPEHQ_raw$doc_id <- as.factor(CEPEHQ_raw$doc_id)
CEPEHQ_raw$doc_id <- plyr::revalue(CEPEHQ_raw$doc_id,
                                              c("P1" = "1"))
```

## Tokenising

A Token is "a meaningful unit of text, most often a word, that we are interested in using for further analysis"
Meaning, for each word we give it a property that we can call upon later. 

The data manipulation for this included removing punctuation, converting to lower-case, and setting word type to word (and not such types as "characters", "ngrams", "sentences", "lines", "regex", "paragraphs")


```{r give text tokens, echo=FALSE, message=FALSE, warning=FALSE}
CEPEHQ_tidy <- CEPEHQ_raw%>% 
  unnest_tokens(word, text)


head(CEPEHQ_tidy)

```

### 2.2 Removing non-alphanumeric characters
* str_extract is used to get rid of non-alphanumeric characters (because we don't want to count _word_ separately from word)
```{r}
CEPEHQ_tidy <- CEPEHQ_tidy %>% 
  mutate(word = str_extract(word, "[a-z']+"))
``` 


### 2.3 Stop words
* Stop words: very common, "meaningless" function words like "the", "of" and "to" -- not usually important in an analysis (i.e. to find out that the most common word in two books you are comparing is "the")
* tidytext has a built-in df called stop_words for English 
* remove these from your dataset with anti_join

We can take a look:
```{r echo=FALSE, message=FALSE, warning=FALSE}
stop_words
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
CEPEHQ_tidy <- CEPEHQ_tidy %>% 
  anti_join(stop_words)

CEPEHQ_tidy
```

Define other stop words:
```{r}
meaningless_words <- tibble(word = c("found", "chatbot", "chatbots", "presentations", "NA", "video", "videos", "didnt", "didn't", "na", "nil", ""))

CEPEHQ_tidy <- CEPEHQ_tidy %>% 
  anti_join(meaningless_words)
```




Break: Prepare your data with the steps above. 1) Unnest tokens, 2) Remove alpha-numeric characters, 3) Remove stopwords 


## 3 Analysing frequencies

### 3.1 Find most frequent words
* Easily find frequent words using count() 
* Data must be in tidy format (one token per line)
* sort = TRUE to show the most frequent words first

tidy_books %>%
  count(word, sort = TRUE) 

```{r this half worked but keep, message=FALSE, warning=FALSE, include=FALSE}

library(tidytext)
library(tidyverse)
library(tm)
library(tigerstats)
library(tidyselect)

CEPEHQ_freq <- CEPEHQ_tidy %>% group_by(doc_id) %>% count(word, sort=TRUE)

CEPEHQ_freq <- CEPEHQ_freq[-6,]

CEPEHQ_freq
```
The preparation of these words has some use in understanding the frequencies, but their emotional valence are not compared. The table above has the word *'helpful'* which has a positive connotation, however there are 386 words, with many having several occurrences. 

What is the sentiment of all participants? What is types of emotional words are being used?


```{r group by word and filter for true, echo=FALSE, message=FALSE, warning=FALSE}
CEPEHQ_tidy %>% 
  group_by(doc_id) %>% 
  count(word, sort=TRUE) %>% 
  filter(doc_id == "1")
```

#### Plotting word frequencies - bar graphs

Bar graph of top words in CEPEHQ.

Basic graph:
```{r eval=FALSE, include=FALSE}
CEPEHQ_freq %>% 
  filter(n>7& doc_id == "1") %>% 
  ggplot(aes(x=word, y=n)) +
  geom_col()
```


```{r Readable labels, echo=FALSE, message=FALSE, warning=FALSE}
CEPEHQ_freq %>% 
  filter(n>7 & doc_id == "1") %>% 
  ggplot(aes(x=word, y=n)) +
  geom_col() +
  theme(axis.text.x = element_text(angle = 45))
```


```{r Descending order, include=FALSE}
CEPEHQ_freq %>% 
  filter(n>7 & doc_id == "1") %>% 
  ggplot(aes(x=reorder(word, -n), y=n)) +
  geom_col() +
  theme(axis.text.x = element_text(angle = 45))
```


```{r include=FALSE}
CEPEHQ_freq %>% 
  filter(n>4 & doc_id == "1") %>% 
  ggplot(aes(x=reorder(word, -n), y=n, fill=n)) +
  geom_col(show.legend=FALSE) +
  theme(axis.text.x = element_text(angle = 45)) +
  xlab("Word") +
  ylab("Frequency") +
  ggtitle("Most frequent words in all CEPEH qualatative data")
```


The words can be grouped and plotted onto a bar chart, and to show more words this chart is presented with horizontal bars. 
The most frequent words present in the various focus group discussions after using the 4 CEPEH chatbots are in the Figure below.


```{r echo=FALSE, message=FALSE, warning=FALSE}
CEPEHQ_freq %>% 
  filter(n>3,doc_id == "1") %>% na.omit(CEPEHQ_freq)%>%
  ggplot(aes(x=reorder(word, n), y=n, fill=n)) +
  geom_col(show.legend=FALSE) +
  xlab("Word") +
  ylab("Frequency") +
  ggtitle("Most frequent words in all CEPEH qualatative data") +
  coord_flip()
```


Although the frequency is not high for each word, we are able to get a general picture of the sentiments, intensities, and concerns which would be immediately occurring when plotted. 



### 3.2 Normalised frequency
* when comparing the frequencies of words from different texts, they are commonly normalised
* convention in corpus linguistics: report the frequency per 1 million words
* for shorter texts: per 10,000 or per 100,000 words
* calculation: raw frequency * 1,000,000 / total numbers in text
```{r}
# see the total number of words per play (doc_id)
CEPEHQ_freq %>% 
  group_by(doc_id) %>% 
  mutate(sum(n)) %>% 
  distinct(doc_id, sum(n))

CEPEHQ_freq <- CEPEHQ_freq %>% 
  na.omit() %>% 
  group_by(doc_id) %>% 
  mutate(pmw = n*2827/sum(n)) %>% # creates a new column called pmw
  ungroup() %>% 
  anti_join(stop_words) # removing stopwords afterwards

CEPEHQ_freq %>% select(word, pmw)
```

#### Plotting normalised frequency
Now we can plot, for example, the 20 most frequent words (by pmw).
```{r CEPEH MOST FREQ, echo=FALSE, message=FALSE, warning=FALSE}
CEPEHQ_freq %>% 
  filter(doc_id == "1") %>% 
  top_n(20, pmw) %>% 
  ggplot(aes(x=reorder(word, -pmw), y=pmw, fill=pmw)) +
  geom_col(show.legend=FALSE) +
  theme(axis.text.x = element_text(angle = 45)) +
  xlab("Word") +
  ylab("Frequency within 2827 words FGD Source") +
  ggtitle("Most frequent words in CEPEH FGDs")

```


### 3.3 Word clouds

Let's visualise the most frequent words in a word cloud. Here, the size indicates the frequency, with words that occur more often being displayed in a larger font size. -this issued to visualise normalised frequency, in accordance to the FGD source document analysed.

```{r echo=FALSE, message=FALSE, warning=FALSE}
wordcloud(words = CEPEHQ_freq$word, freq = CEPEHQ_freq$n, 
          min.freq = 4, max.words=2000, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))
```
We understand the context has been reduced for each word. However, in general there can be categorised positive/negative words from the word cloud: 
Positive words are- benefit, practical, nice, helpful, learn, ideas, and enjoyed
Negative words are- difficult, test (who likes a test?), don't, and 'lot' may be negative if there is a 'lot' of information.


## The vocabulary of Texts

```{r include=FALSE}
comp_2 <- CEPEHQ_freq %>% 
  filter(doc_id == "1"|doc_id == "1") %>% 
  group_by(doc_id) %>% 
  mutate(proportion = n / sum(n)) %>% #creates proportion column (word frequency divided by overall frequency per author)
  select(-n) %>%
  spread(doc_id, proportion)

summary(comp_2)
```

Here is a graph that has plotted the words in places depending on the word frequencies. Additionally, colour coding shows how different the frequencies are - darker items are more similar in terms of their frequencies, lighter-coloured ones more frequent in one text compared to the other.
```{r echo=FALSE, message=FALSE, warning=FALSE}
ggplot(comp_2, 
       aes(x = `1`, y = `1`, 
           color = abs(`1` - `1`))) +
  geom_abline(color = "blue1", lty = 2) +
  geom_jitter(alpha = 0.1, size = 4.5, width = 0.5, height = 0.8) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  theme_light() +
  theme(legend.position="none") +
  labs(y = "1", x = "1")
```



## Sentiment analysis

```{r sentiment, echo=FALSE, message=FALSE, warning=FALSE}
head(get_sentiments("bing"))

CEPEHQ_tidy %>% 
  inner_join(get_sentiments("bing")) %>% 
  head()

CEPEHQ_tidy %>% 
  inner_join(get_sentiments("bing"))%>% 
  count(sentiment)

CEPEHQ_tidy %>% 
  inner_join(get_sentiments("bing")) %>% 
  count(word, sentiment, sort = TRUE) %>% 
  head()

CEPEHQ_tidy %>% 
  inner_join(get_sentiments("bing")) %>% 
  count(doc_id,sentiment) %>% 
  head()


CEPEHQ_tidy  %>% 
  inner_join(get_sentiments("bing"))%>% 
  count(sentiment)

CEPEHQ_tidy  %>% 
  inner_join(get_sentiments("bing")) %>% 
  count(word, sentiment, sort = TRUE) %>% 
  head()

```

```{r echo=FALSE, message=FALSE, warning=FALSE}

CEPEHQ_tidy_bing <-  CEPEHQ_tidy%>% 
  inner_join(get_sentiments("bing")) %>% 
  count(sentiment) %>% 
  spread(sentiment, n, fill=0) %>% 
  mutate(total_score = positive - negative)
```

```{r sentiment2, echo=TRUE, message=FALSE, warning=FALSE}
CEPEHQ_tidy_bing %>% 
  summarize(max(total_score), min(total_score)) 


knitr::kable(CEPEHQ_tidy_bing) 

```


```{r echo=FALSE, message=FALSE, warning=FALSE, out.height='1000px', out.width='100%'}
knitr::include_graphics("figures/sample-content/pdf_embed_example/Lyngs2020_FB.pdf")
```
<br>


```{r echo=FALSE, message=FALSE, warning=FALSE, linewidth=, results='asis'}
# install.packages(pdftools)
# split PDF into pages stored in figures/sample-content/pdf_embed_example/split/
# pdftools::pdf_split("figures/sample-content/pdf_embed_example/Lyngs2020_FB.pdf",
#        output = "figures/sample-content/pdf_embed_example/split/")

# grab the pages
pages <- list.files("figures/sample-content/pdf_embed_example/split", full.names = TRUE)

# set how wide you want the inserted PDFs to be: 
# 1.0 is 100 per cent of the oxforddown PDF page width;
# you may want to make it a bit bigger
pdf_width <- 1.2

# for each PDF page, insert it nicely and
# end with a page break
cat(stringr::str_c("\\newpage \\begin{center} \\makebox[\\linewidth][c]{\\includegraphics[width=", pdf_width, "\\linewidth]{", pages, "}} \\end{center}"))

```


```{r echo=FALSE, message=FALSE, warning=FALSE, out.height='1000px', out.width='100%'}
knitr::include_graphics("figures/sample-content/alt_frontmatter_example/alt-frontmatter-example.pdf")
```

\noindent
```{r echo=FALSE, message=FALSE, warning=FALSE, out.height='1000px', out.width='100%', results='asis'}
# grab the pages
pages <- list.files("figures/sample-content/alt_frontmatter_example/split", full.names = TRUE)

pdf_width <- 0.32

cat(stringr::str_c("\\fbox{\\includegraphics[width=", pdf_width, "\\linewidth]{", pages, "}}"))

```
