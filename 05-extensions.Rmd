---
output:
  #bookdown::html_document2: default
  #bookdown::word_document2: default
  bookdown::pdf_document2: 
    template: templates/template.tex
documentclass: book
#bibliography: [bibliography/references.bib, bibliography/additional-references.bib]
---

```{block type='savequote', include=knitr::is_latex_output(), quote_author='(ref:darwin-quote)'}
There is grandeur in this view of life, with its several powers, having been originally breathed into a few forms or into one; and that, whilst this planet has gone cycling on according to the fixed law of gravity, from so simple a beginning endless forms most beautiful and most wonderful have been, and are being, evolved.
```
(ref:darwin-quote) --- Charles Darwin [@Darwin1859]
<!-- note that the quote author won't show up when you knit just a single chapter -->
  
# Text Mining, Natural Language Processing, and Sentiment Analysis



```{r message=FALSE, warning=FALSE}
library(tidyverse) #for various data manipulation tasks
library(tidytext) #for text mining specifically, main package in book
library(stringr) #for various text operations
library(gutenbergr) #to access full-text books that are in the public domain
library(scales) # for visualising percentages
library(readtext) # for reading in txt files
library(wordcloud) # for creating wordclouds
```


## 1 Reading in texts

### 1.1 txt files
Here's how you can read in one .txt file that is saved in the same location as this script (i.e. in the same folder on your computer):
```{r}
CEPEHQ_raw <- readtext("P1.txt")

CEPEHQ_raw $doc_id <- sub(".txt", "", CEPEHQ_raw $doc_id) # this gets rid of .txt in the play titles
```

If you want to read all files from a sub-folder, type the name of the folder followed by / and * to ask R to read in all files in that folder:
```{r eval=FALSE, include=FALSE}
CEPEHQ_raw <- readtext("Shakespeare txts/*")

```




### 1.3 Preparing data
- convert name to ID numbers with more descriptive labels 
```{r}
CEPEHQ_raw$doc_id <- as.factor(CEPEHQ_raw$doc_id)
CEPEHQ_raw$doc_id <- plyr::revalue(CEPEHQ_raw$doc_id,
                                              c("P1" = "1"))
```



## 2 Tidy text
* One word per row, facilitates analysis
* Token: "a meaningful unit of text, most often a word, that we are interested in using for further analysis"

### 2.1 the unnest_tokens function
* Easy to convert from full text to token per row with unnest_tokens()
Syntax: unnest_tokens(df, newcol, oldcol)
* unnest_tokens() automatically removes punctuation and converts to lowercase (unless you set to_lower = FALSE)
* by default, tokens are set to words, but you can also use token = "characters", "ngrams", "sentences", "lines", "regex", "paragraphs", and even "tweets" (which will retain usernames, hashtags, and URLs)

```{r}
CEPEHQ_tidy <- CEPEHQ_raw%>% 
  unnest_tokens(word, text)


CEPEHQ_tidy

```

### 2.2 Removing non-alphanumeric characters
* str_extract is used to get rid of non-alphanumeric characters (because we don't want to count _word_ separately from word)
```{r}
CEPEHQ_tidy <- CEPEHQ_tidy %>% 
  mutate(word = str_extract(word, "[a-z']+"))
``` 


### 2.3 Stop words
* Stop words: very common, "meaningless" function words like "the", "of" and "to" -- not usually important in an analysis (i.e. to find out that the most common word in two books you are comparing is "the")
* tidytext has a built-in df called stop_words for English 
* remove these from your dataset with anti_join

We can take a look:
```{r}
stop_words
```

```{r}
CEPEHQ_tidy <- CEPEHQ_tidy %>% 
  anti_join(stop_words)

CEPEHQ_tidy
```

Define other stop words:
```{r}
meaningless_words <- tibble(word = c("found", "chatbot", "chatbots", "presentations", "NA", "video", "videos", "didnt", "na", "nil", ""))

CEPEHQ_tidy <- CEPEHQ_tidy %>% 
  anti_join(meaningless_words)
```




Break: Prepare your data with the steps above. 1) Unnest tokens, 2) Remove alpha-numeric characters, 3) Remove stopwords 


## 3 Analysing frequencies

### 3.1 Find most frequent words
* Easily find frequent words using count() 
* Data must be in tidy format (one token per line)
* sort = TRUE to show the most frequent words first

tidy_books %>%
  count(word, sort = TRUE) 

```{r}

CEPEHQ_freq <- CEPEHQ_tidy %>% 
  group_by(doc_id) %>% #including this ensures that the counts are by book and the id column is retained
  count(word, sort=TRUE)
CEPEHQ_freq <- CEPEHQ_freq[-6,]

CEPEHQ_freq
```




```{r}
CEPEHQ_tidy %>% 
  group_by(doc_id) %>% 
  count(word, sort=TRUE) %>% 
  filter(doc_id == "1")
```

#### Plotting word frequencies - bar graphs

Bar graph of top words in CEPEHQ.

Basic graph:
```{r}
CEPEHQ_freq %>% 
  filter(n>7& doc_id == "1") %>% 
  ggplot(aes(x=word, y=n)) +
  geom_col()
```

Readable labels:
```{r}
CEPEHQ_freq %>% 
  filter(n>7 & doc_id == "1") %>% 
  ggplot(aes(x=word, y=n)) +
  geom_col() +
  theme(axis.text.x = element_text(angle = 45))
```

Descending order:
```{r}
CEPEHQ_freq %>% 
  filter(n>7 & doc_id == "1") %>% 
  ggplot(aes(x=reorder(word, -n), y=n)) +
  geom_col() +
  theme(axis.text.x = element_text(angle = 45))
```

Axis names and colors:
```{r}
CEPEHQ_freq %>% 
  filter(n>7 & doc_id == "1") %>% 
  ggplot(aes(x=reorder(word, -n), y=n, fill=n)) +
  geom_col(show.legend=FALSE) +
  theme(axis.text.x = element_text(angle = 45)) +
  xlab("Word") +
  ylab("Frequency") +
  ggtitle("Most frequent words in all CEPEH qualatative data")
```

Or: flip coordinate system to make more space for words
```{r}
CEPEHQ_freq %>% 
  filter(n>7,doc_id == "1") %>% 
  ggplot(aes(x=reorder(word, n), y=n, fill=n)) +
  geom_col(show.legend=FALSE) +
  xlab("Word") +
  ylab("Frequency") +
  ggtitle("Most frequent words in all CEPEH qualatative data") +
  coord_flip()
```

### 3.2 Normalised frequency
* when comparing the frequencies of words from different texts, they are commonly normalised
* convention in corpus linguistics: report the frequency per 1 million words
* for shorter texts: per 10,000 or per 100,000 words
* calculation: raw frequency * 1,000,000 / total numbers in text
```{r}
# see the total number of words per play (doc_id)
CEPEHQ_freq %>% 
  group_by(doc_id) %>% 
  mutate(sum(n)) %>% 
  distinct(doc_id, sum(n))

CEPEHQ_freq <- CEPEHQ_freq %>% 
  na.omit() %>% 
  group_by(doc_id) %>% 
  mutate(pmw = n*1000000/sum(n)) %>% # creates a new column called pmw
  ungroup() %>% 
  anti_join(stop_words) # removing stopwords afterwards

CEPEHQ_freq %>% select(word, pmw)
```

#### Plotting normalised frequency
Now we can plot, for example, the 20 most frequent words (by pmw).
```{r}
CEPEHQ_freq %>% 
  filter(doc_id == "1") %>% 
  top_n(20, pmw) %>% 
  ggplot(aes(x=reorder(word, -pmw), y=pmw, fill=pmw)) +
  geom_col(show.legend=FALSE) +
  theme(axis.text.x = element_text(angle = 45)) +
  xlab("Word") +
  ylab("Frequency per 1 million words") +
  ggtitle("Most frequent words in HELM")
```


### 3.3 Word clouds

Let's visualise the most frequent words in a word cloud. Here, the size indicates the frequency, with words that occur more often being displayed in a larger font size, but this can also be used to visualise e.g. normalised frequency (pmw) or length or anything else you pass to the freq = part of the command.
```{r}
wordcloud(words = CEPEHQ_freq$word, freq = CEPEHQ_freq$n, 
          min.freq = 6, max.words=2000, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))
```


## 4 Comparing the vocabulary of texts

Next, we'll create two graphs to compare the vocabulary of our texts. First, we focus on Alice's Adventures and Anderson's CEPEHQ. The newly created comp_2 data frame contains only the words and their frequencies in the two texts in two separate columns.

### Comparing two texts
```{r}
comp_2 <- CEPEHQ_freq %>% 
  filter(doc_id == "1"|doc_id == "1") %>% 
  group_by(doc_id) %>% 
  mutate(proportion = n / sum(n)) %>% #creates proportion column (word frequency divided by overall frequency per author)
  select(-n) %>%
  spread(doc_id, proportion)

head(comp_2)
```

Now, we can plot the words. Their placement depends on the word frequencies. Additionally, colour coding shows how different the frequencies are - darker items are more similar in terms of their frequencies, lighter-coloured ones more frequent in one text compared to the other. We'll discuss the interpretation in more detail once we've created the threeway comparison.
```{r}
ggplot(comp_2, 
       aes(x = `1`, y = `1`, 
           color = abs(`1` - `1`))) +
  geom_abline(color = "gray40", lty = 2) +
  geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  theme_light() +
  theme(legend.position="none") +
  labs(y = "1", x = "1")
```









```{r, eval=knitr::is_html_output(), out.width='100%', out.height='1000px'}
knitr::include_graphics("figures/sample-content/pdf_embed_example/Lyngs2020_FB.pdf")
```
<br>


```{r, echo=TRUE, results='asis', eval=knitr::is_latex_output(), linewidth = 70}
# install.packages(pdftools)
# split PDF into pages stored in figures/sample-content/pdf_embed_example/split/
# pdftools::pdf_split("figures/sample-content/pdf_embed_example/Lyngs2020_FB.pdf",
#        output = "figures/sample-content/pdf_embed_example/split/")

# grab the pages
pages <- list.files("figures/sample-content/pdf_embed_example/split", full.names = TRUE)

# set how wide you want the inserted PDFs to be: 
# 1.0 is 100 per cent of the oxforddown PDF page width;
# you may want to make it a bit bigger
pdf_width <- 1.2

# for each PDF page, insert it nicely and
# end with a page break
cat(stringr::str_c("\\newpage \\begin{center} \\makebox[\\linewidth][c]{\\includegraphics[width=", pdf_width, "\\linewidth]{", pages, "}} \\end{center}"))

```


```{r, echo=FALSE, eval=knitr::is_html_output(), out.width='100%', out.height='1000px'}
knitr::include_graphics("figures/sample-content/alt_frontmatter_example/alt-frontmatter-example.pdf")
```

\noindent
```{r, echo=FALSE, out.width='100%', out.height='1000px', results='asis', eval=knitr::is_latex_output}
# grab the pages
pages <- list.files("figures/sample-content/alt_frontmatter_example/split", full.names = TRUE)

pdf_width <- 0.32

cat(stringr::str_c("\\fbox{\\includegraphics[width=", pdf_width, "\\linewidth]{", pages, "}}"))
```